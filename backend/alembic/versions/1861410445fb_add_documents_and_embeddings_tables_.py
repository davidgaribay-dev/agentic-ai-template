"""add documents and embeddings tables with pgvector

Revision ID: 1861410445fb
Revises: 1453168337f4

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
import sqlmodel
from sqlalchemy.dialects import postgresql
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision: str = '1861410445fb'
down_revision: Union[str, Sequence[str], None] = '1453168337f4'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # Enable pgvector extension for vector similarity search
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('documents',
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.Column('organization_id', sa.Uuid(), nullable=False),
    sa.Column('team_id', sa.Uuid(), nullable=True),
    sa.Column('user_id', sa.Uuid(), nullable=True),
    sa.Column('created_by_id', sa.Uuid(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('filename', sqlmodel.sql.sqltypes.AutoString(length=255), nullable=False),
    sa.Column('file_path', sqlmodel.sql.sqltypes.AutoString(length=512), nullable=False),
    sa.Column('file_size', sa.Integer(), nullable=False),
    sa.Column('file_type', sqlmodel.sql.sqltypes.AutoString(length=10), nullable=False),
    sa.Column('mime_type', sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
    sa.Column('processing_status', sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
    sa.Column('processing_error', sa.Text(), nullable=True),
    sa.Column('chunk_count', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['created_by_id'], ['user.id'], ondelete='SET NULL'),
    sa.ForeignKeyConstraint(['organization_id'], ['organization.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['team_id'], ['team.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_documents_org_team_user', 'documents', ['organization_id', 'team_id', 'user_id'], unique=False)
    op.create_index('idx_documents_status', 'documents', ['processing_status'], unique=False)
    op.create_index(op.f('ix_documents_deleted_at'), 'documents', ['deleted_at'], unique=False)
    op.create_index(op.f('ix_documents_filename'), 'documents', ['filename'], unique=False)
    op.create_index(op.f('ix_documents_processing_status'), 'documents', ['processing_status'], unique=False)
    op.create_table('document_chunks',
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('document_id', sa.Uuid(), nullable=False),
    sa.Column('organization_id', sa.Uuid(), nullable=False),
    sa.Column('team_id', sa.Uuid(), nullable=True),
    sa.Column('user_id', sa.Uuid(), nullable=True),
    sa.Column('chunk_index', sa.Integer(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('token_count', sa.Integer(), nullable=True),
    sa.Column('embedding', Vector(1536), nullable=True),
    sa.Column('metadata_', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_chunks_document', 'document_chunks', ['document_id'], unique=False)
    op.create_index('idx_chunks_tenant', 'document_chunks', ['organization_id', 'team_id', 'user_id'], unique=False)
    op.create_index(op.f('ix_document_chunks_document_id'), 'document_chunks', ['document_id'], unique=False)
    op.create_index(op.f('ix_document_chunks_organization_id'), 'document_chunks', ['organization_id'], unique=False)
    op.create_index(op.f('ix_document_chunks_team_id'), 'document_chunks', ['team_id'], unique=False)
    op.create_index(op.f('ix_document_chunks_user_id'), 'document_chunks', ['user_id'], unique=False)

    # Create HNSW index for fast vector similarity search
    # m=16: Number of connections per layer (balance between memory and search quality)
    # ef_construction=64: Build quality parameter (higher = better recall, slower build)
    op.execute("""
        CREATE INDEX idx_document_chunks_embedding_hnsw
        ON document_chunks
        USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64)
    """)

    # NOTE: Do not drop LangGraph checkpoint tables (managed by LangGraph, not SQLModel)
    # These tables are: checkpoints, checkpoint_blobs, checkpoint_writes, checkpoint_migrations
    # And: store, store_vectors, store_migrations, vector_migrations
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Drop vector index
    op.execute("DROP INDEX IF EXISTS idx_document_chunks_embedding_hnsw")

    # Drop document tables
    op.drop_index(op.f('ix_document_chunks_user_id'), table_name='document_chunks')
    op.drop_index(op.f('ix_document_chunks_team_id'), table_name='document_chunks')
    op.drop_index(op.f('ix_document_chunks_organization_id'), table_name='document_chunks')
    op.drop_index(op.f('ix_document_chunks_document_id'), table_name='document_chunks')
    op.drop_index('idx_chunks_tenant', table_name='document_chunks')
    op.drop_index('idx_chunks_document', table_name='document_chunks')
    op.drop_table('document_chunks')

    op.drop_index(op.f('ix_documents_processing_status'), table_name='documents')
    op.drop_index(op.f('ix_documents_filename'), table_name='documents')
    op.drop_index(op.f('ix_documents_deleted_at'), table_name='documents')
    op.drop_index('idx_documents_status', table_name='documents')
    op.drop_index('idx_documents_org_team_user', table_name='documents')
    op.drop_table('documents')
    # ### end Alembic commands ###
